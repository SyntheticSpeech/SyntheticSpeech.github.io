<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docs on Synthetic Audio Generation</title>
    <link>https://syntheticspeech.github.io/docs/</link>
    <description>Recent content in Docs on Synthetic Audio Generation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Nov 2023 14:54:44 -0700</lastBuildDate>
    <atom:link href="https://syntheticspeech.github.io/docs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deepfakes, Then and Now</title>
      <link>https://syntheticspeech.github.io/docs/deepfakes-history/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://syntheticspeech.github.io/docs/deepfakes-history/</guid>
      <description>Deepfakes, Then and Now # Deepfake videos and images that convincingly replace one person&amp;rsquo;s likeness with another&amp;rsquo;s, have rapidly grown in popularity and sophistication. The generation of deepfakes for both benign and nefarious purposes has only become more accessible through apps like FakeApp, Synthesia, Deepfakesweb.com and more. As a result, research into deepfakes has surged, driven by the need to understand, detect, and mitigate the potential threats they pose.</description>
    </item>
    <item>
      <title>Our Research</title>
      <link>https://syntheticspeech.github.io/docs/our-research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://syntheticspeech.github.io/docs/our-research/</guid>
      <description>Our Research # VITS # VITS is a previous SOTA model. It is the first end-to-end model compare to previous 2-stage model, meaning there is no need to produce intermediate mel-spectrums.
Our fine-tuning code is based on this repo.
The repo has a pretrained model on VCTK and some Animes. It can be used for fine-tuning Chinese, Japanese and English.
We self recorded a tiny collection of voices, and tried to fine-tune pretrained VITS model based on this dataset.</description>
    </item>
    <item>
      <title>Samples</title>
      <link>https://syntheticspeech.github.io/docs/samples/</link>
      <pubDate>Wed, 01 Nov 2023 14:54:44 -0700</pubDate>
      <guid>https://syntheticspeech.github.io/docs/samples/</guid>
      <description> Samples # We present the results of our work below, categorized based on the models we ran and the different vocal features we were testing for.
XTTS vs. VALL-E-X # Custom Voice Input # Input voice: XTTS VALL-E-X </description>
    </item>
    <item>
      <title>References</title>
      <link>https://syntheticspeech.github.io/docs/references/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://syntheticspeech.github.io/docs/references/</guid>
      <description>References # *Draft* # [1] A. van den Oord et al., “WaveNet: A Generative Model for Raw Audio,” 2016, doi: 10.48550/ARXIV.1609.03499.
[2] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High Fidelity Neural Audio Compression,” 2022, doi: 10.48550/ARXIV.2210.13438.
[3] W. Ping et al., “Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,” 2017, doi: 10.48550/ARXIV.1710.07654.
[4] X. Tan, T. Qin, F. Soong, and T.-Y. Liu, “A Survey on Neural Speech Synthesis,” 2021, doi: 10.</description>
    </item>
    <item>
      <title>Meet the Team</title>
      <link>https://syntheticspeech.github.io/docs/team/</link>
      <pubDate>Wed, 01 Nov 2023 14:54:44 -0700</pubDate>
      <guid>https://syntheticspeech.github.io/docs/team/</guid>
      <description>Meet the Team # *Draft* # TODO: Images, linkedin contact and info
As part of a collboration between SLAC, CMU and Stanford.
SLAC # Sponsor point(s) of contact: Mayank Malik (mayank13@stanford.edu)
CMU Faculty # Special Thanks to CMU Faculty
David Varodayan (dvaroday@andrew.cmu.edu) Cynthia Kuo Sujata Telang Gladys Mercier CMU Student Team # Technical lead(s): Nivea Sharma (niveas@andrew.cmu.edu) Engineering Team:
Hao Ren (haoren@andrew.cmu.edu) Sivani Papini (spapini@andrew.cmu.edu) Thenuga Priyadharshini (tpriyadh@andrew.cmu.edu) Tianyi Li (tianyil3@andrew.</description>
    </item>
  </channel>
</rss>
