[{"id":0,"href":"/docs/samples/xtts_vallex/","title":"XTTS vs. VALL-E-X","section":"Samples","content":" XTTS vs. VALL-E-X # We present the results of our work below, categorized based on the models we ran and the different vocal features we were testing for.\nCustom Voice Input # Input voice: XTTS VALL-E-X Voice with Emotion # These samples demonstrate how XTTS and VALL-E X retain the emotion of the input voice.\nInput voice (Emotion: Amused): XTTS VALL-E-X Input voice (Emotion: Anger): XTTS VALL-E-X Input voice (Emotion: Disgust): XTTS VALL-E-X Voice with Environmental Noise and Textures # These samples demonstrate how XTTS and VALL-E X handle voice inputs that have an environment or texture that causes the voice to sound like it has a filter.\nInput voice (Spoken through a phone): XTTS VALL-E-X Input Voice from LibriSpeech Dataset # XTTS VALL-E-X Input Voice from VCTK Dataset # XTTS VALL-E-X "},{"id":1,"href":"/docs/deepfakes-history/","title":"Deepfakes, Then and Now","section":"Docs","content":" Deepfakes, Then and Now # Deepfake videos and images that convincingly replace one person\u0026rsquo;s likeness with another\u0026rsquo;s, have rapidly grown in popularity and sophistication. The generation of deepfakes for both benign and nefarious purposes has only become more accessible through apps like FakeApp, Synthesia, Deepfakesweb.com and more. As a result, research into deepfakes has surged, driven by the need to understand, detect, and mitigate the potential threats they pose.\nSeveral key factors drive increased interest in deepfakes research:\nAdvancements in AI and Deep Learning: Breakthroughs in artificial intelligence and deep learning have made it easier to create highly convincing deepfakes, necessitating a parallel advancement in detection and prevention methods. High-Profile Incidents: Deepfake-related incidents involving public figures and politicians have heightened awareness about their potential for malicious use, prompting more researchers to address these concerns. Privacy Concerns: As deepfake technology can be misused to create non-consensual explicit content or deceive individuals, privacy advocates have called for more research and legislation to combat these issues. National Security: Governments and security agencies have recognized the potential threat deepfakes pose to national security and infrastructure, leading to increased funding for research efforts to develop countermeasures. The landscape for deepfake research is expanding, taking root in disciplines outside of pure information and computer science:\nMitigation strategies for the proliferation of deepfakes requires a potentially interdisciplinary approach, seeing that it bleeds into every facet of society:\nDetection and Authentication: Researchers are developing tools and algorithms to identify deepfake content, often based on discrepancies in facial movements, blinking patterns, or audio inconsistencies. Policy and Legislation: Scholars are investigating the legal and ethical implications of deepfakes and advocating for regulations to prevent their misuse. Media Literacy and Enhanced Communication Skills: Efforts are underway to educate the public about deepfakes and provide tools for recognizing them, enhancing digital literacy. Erosion in public trust of important news sources and previously established language and communication rules is extremely detrimental, and deepfakes only exacerbate the issue. Countermeasures: Technological countermeasures, such as watermarking and authentication techniques, are being developed to hinder the creation and distribution of malicious deepfake content. Cross-Disciplinary Collaboration: Researchers from fields as diverse as computer science, psychology, and ethics are joining forces to address the multifaceted challenges posed by deepfakes. The rise of deepfake technology has prompted a commensurate increase in research, as the potential consequences of its misuse become more apparent. Deepfake research is essential to stay ahead of malicious uses and protect privacy, security, and societal trust. As this technology continues to advance, collaboration between academia, industry, and government will be vital in our ongoing battle against the deepfake phenomenon.\n"},{"id":2,"href":"/docs/related_work/zero-shot-tts/","title":"Zero-Shot TTS","section":"Related Work","content":" Zero-Shot TTS # The next leap made for the generation of deepfaked audios is zero-shot TTSes. Zero-shot text-to-speech (TTS) refers to the capability of a system to generate speech in a target voice without any specific training on that particular voice. In a traditional TTS system, training involves using a large dataset of recordings from a specific speaker to learn the characteristics of that speaker\u0026rsquo;s voice. In contrast, zero-shot TTS aims to generate speech in a new, unseen voice without requiring explicit training on that voice.\nYourTTS # YourTTS has been shown to achieve state-of-the-art results on the VCTK dataset, and it is a promising new approach to zero-shot multi-speaker TTS. Find the paper here.\nXTTS # XTTS is a cross-lingual text-to-speech synthesis and speech-to-speech translation model that is based on VALL-E. It uses a number of modifications to VALL-E, including a multilingual transformer encoder, a cross-lingual attention mechanism, and a multilingual audio codec model. XTTS has been shown to achieve state-of-the-art results on the Multilingual VCTK dataset and has the potential to be used for a variety of applications.\n"},{"id":3,"href":"/docs/our-research/","title":"Our Research","section":"Docs","content":" Our Research # VITS # VITS is a previous SOTA model. It is the first end-to-end model compare to previous 2-stage model, meaning there is no need to produce intermediate mel-spectrums.\nOur fine-tuning code is based on this repo.\nThe repo has a pretrained model on VCTK and some Animes. It can be used for fine-tuning Chinese, Japanese and English.\nWe self recorded a tiny collection of voices, and tried to fine-tune pretrained VITS model based on this dataset. Besides, we also tried public dataset like LibriSpeech, and publically acquired voice samples like Anne Hathaway\u0026rsquo;s speech. Our experiment records are shown as follow:\nDataset Epoch Result* Note Hao\u0026rsquo;s self collected voices (19 Samples) 20 Tune close, pronounciation fail n/a Hao\u0026rsquo;s self collected voices (41 Samples) 25 No improvement n/a Anne Hathaway, 12 mins audio 20 Tune close, pronounciation better, not perfect n/a Hao\u0026rsquo;s self collected voices (15 mins audio, Chinese) 50 Tune close, still strange pronouns, but better than EN n/a LibriSpeech(Speaker 3572, 103 samples) 50 Pretty steady n/a *Since performance of synthetic audios are usually evaluated by humans (like MOS), result column records our experience with the output audio.\nWe found that the quality of the fine-tuned model is highly dependend on number of data samples. A successfully fine-tuned model needs hours of audio samples.\nYourTTS # A newer ZS-TTS model based on VITS, proposed by Coqui. It is also multi-lingual. We implemented our fine-tuning based on the open-source code released by Coqui.\nDataset Epoch Result Note Hao\u0026rsquo;s self collected voices (19 Samples) 20 fail, voice not clearly formed n/a Hao\u0026rsquo;s self collected voices (19 Samples) 160 fail, voice not clearly formed n/a We did not dig deeper into the training, as we found VALL-E model later and shifted our priority to VALL-E model.\nVALL-E # Microsoft\u0026rsquo;s new SOTA ZS-TTS model that proposed a different way to perform the task. It tokenize the audio prompt and text prompt into T * 8 matrix, and then feed them into an Auto-regressive model and a Non-autogressive model.\nFor each model, the basic strcuture contains several embedding layers, and a Transformer Decoder layer.\nSince Microsoft does not publish VALL-E\u0026rsquo;s code, we tried several publically avaiable implementation of Microsoft VALL-E, described in following sections.\nenhuiz # This repo uses g2p (Grapheme-to-phoneme conversion) to quantize and tokenize dataset. It also uses Deepspeed to speed up training.\nThe main issue with this method is that there is no pretrained model. So we tried to train a backbone model using LibriLight, and the fine-tune the backbone model using self-collected smaller dataset.\nProbelms:\nTraining a sophisticated backbone model ourselves requires way more time and knowledge. LibriLight is too small (1200 samples) for backbone model. Directly trained on self-collected dataset, gives toy result as exepcted. Ecker # Ecker\u0026rsquo;s code is based on enhuiz\u0026rsquo;s scripts, but the author released a pretrained model. So we thought we can conduct our fine-tuning based on the pretrained model.\nProblems:\npretrained model has error, it only gives nan result in AR forward. We also checked on huggingface, the model is not working. Training based on the pretrained model has a high loss. Lifeiteng # Lifeiteng\u0026rsquo;s implementation relies on Lhotse for its dataset preparation. Lhotse will process the audios into CutSet, and later a DynamicBucketSampler is used for constructing the dataloader.\nOne interesting bug we found is that we need to change our dataset to mono-channel, since Encodec uses 24KHZ mono-channel. If stereo inputs are given, we need to use 49KHZ Encodec model.\nNo pretrained model is released for Lifeiteng\u0026rsquo;s repo. We found a pioneer who trained on LibriTTS for 100 epoches, and continued our fine-tuning based on his checkpoint.\nProblem:\nThe checkpoint performance is very bad. On AWS, we met complex version problem brought by torch, nnCUDA, K2 and python version. Seems no easy workable solution can be found among them. Plachtaa # Checkout our fine-tune branch.\nPlachtaa uses Lifeiteng\u0026rsquo;s repo, and releases a super well-trained pretrained model. The only problem is that Plachtaa deleted all traing related codes, like dataset processing, training script and even model\u0026rsquo;s forward pass.\nThere are differences between Plachtaa and Lifeiteng\u0026rsquo;s implementation: Plachtaa supports multi-lingual by adding language embedding, uses different tokenizer and trained on different dataset.\nWe tried to re-write all these missing training codes, and runned our fine-tuning. Following sections summaries our experiments. We are still facing inference issue as generated audio is mainly noise, actively solving it now.\nImplementation details # As lifeiteng uses Lhotse, we have to make our dataset something like libriTTS. We also need to use its CutSet as dataset class, and its Sampler when constructing dataloader.\nLifeiteng and Plachtaa uses different tokenizer, resulting difference in TextTokenCollator\nLifeiteng Tokenize logic: text_prompt -\u0026gt; tokenizer (espeak) -\u0026gt; phonemes -\u0026gt; Collator (using precomputed SymbolTable) -\u0026gt; index sequence\nPlachtaa Tokenize logic: text_prompt -\u0026gt; Add language ID -\u0026gt; tokenizer (PhonemeBpeTokenizer, with pre-defined index mapping) -\u0026gt; index sequence directly, Collator just need to perform EOS/DOS/PAD\nThe most important thing to notice is that Plachtaa added language ID to text prompt:\ntext = \u0026#34;[EN]\u0026#34; + text + \u0026#34;[EN]\u0026#34; Besides, phoneme index mapping is different. For example, PAD in PhonemeBpeTokenizer is 3, but in Lifeiteng\u0026rsquo;s Tokenizer is 0.\nIn the model\u0026rsquo;s forward pass, we added language embedding calculation accordingly.\nFirst run # Successfully trained on Plachtaa\u0026rsquo;s pretrained model, using Lifeiteng\u0026rsquo;s training code. Several key things to notice:\nPrepare our dataset in LibriTTS format Using Lifeiteng\u0026rsquo;s VALLE forward, but added language embedding Fixed embedding size in order to load Plachtaa\u0026rsquo;s pretrained model Fixed erros in Checkpoint, training logic, data processing (collation), etc. Fixed inference script: it correctly produces good result if we directly use pretrained model Run 2 # learning rate 1e-5 warmup-epochs 0 Trained AR, epoch 80, keep ar_predict_layer unfreezed log_interval, valid_interval, checkpoint frequency and logic Phase loss Top10Accuracy Train 4.657 0.5279 Valid 4.909 0.4687 Run 3 # Add language id \u0026ldquo;EN\u0026rdquo; to transcription, change to PhonemeBpeTokenizer text tokenizer in prepare stage learning rate 1e-6 warmup-epochs 0 Trained AR, epoch 80, keep ar_predict_layer unfreezed Trained NAR epoch 180, keep nar_predict_layers unfreezed Phase loss Top10Accuracy AR Train 4.967 0.4304 AR Valid 4.914 0.4665 NAR Train 7.999 0.01076 NAR Valid 7.79 0.01838 Run 4 # Fixed wrong embedding used in AR forwarding learning rate 0.04 (ScaledAdam should be smart enough) warmup-epochs 200 Trained AR, epoch 160, keep ar_predict_layer unfreezed Phase loss Top10Accuracy AR Train 4.765 0.4365 AR Valid 4.763 0.4665 Run 5 # Fixed forward logic: removed enrolled_len learning rate 1e-4, ScaledAdam warmup-epochs 200 Trained AR, epoch 1000, keep ar_predict_layer unfreezed Phase loss Top10Accuracy AR Train 4.396 0.4497 AR Valid 4.504 0.4697 Run 6 # libriTTS dataset: dev-clean, test-clean (train cut 08:58:13, dev cut 00:47:31, test cut 08:34:09) learning rate 1e-5, ScaledAdam, warmup-epochs 200 AR, 20 epochs, (did not include validation as valid_interval was wrong) Phase loss Top10Accuracy AR Train 5.748 0.1868 Run 7 # learning rate 0.04, ScaledAdam, warmup-epochs 200 AR 20 epochs, NAR 20 epochs Phase loss Top10Accuracy AR Train 5.748 0.1868 AR Valid 5.923 0.1715 NAR Train 5.669 0.2045 NAR Valid 5.759 0.1858 "},{"id":4,"href":"/docs/related_work/valle/","title":"VALL-E","section":"Related Work","content":" VALL-E # VALL-E is a neural codec language model for text-to-speech (TTS) synthesis that can generate high-quality speech that is indistinguishable from human speech. It is trained on a massive dataset of text and audio, and can also clone speakers\u0026rsquo; voices, even from a short sample of their speech. VALL-E works by first converting text into a sequence of discrete codes. These codes are then used to generate a corresponding sequence of audio tokens. The audio tokens are then decoded into a waveform using a neural audio codec model.\nKey features # Uses a language modeling approach and applies it to TTS. Train a neural codec language model using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression. The model takes in text prompts as phonemes and a 3-second recording sample from the target speaker. The model then generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second recording and the phoneme prompt. Finally, the generated acoustic tokens are used to synthesize the final waveform with corresponding neural codec decoder. Ultimately, VALL-E generates synthetic, natural-sounding speech with high degree of similarity to the speaker with zero-shot prompting, outperforming YourTTS on LibriSpeech and VCTK.\nVALL-E X # VALL-E X is an extension of VALL-E that enables cross-lingual text-to-speech synthesis and speech-to-speech translation. It is trained on a multilingual dataset of text and audio, and can synthesize speech in a target language from a text prompt in the source language, while preserving the speaker\u0026rsquo;s voice, emotion, and acoustic environment.YourTTSYourTTS is a zero-shot multi-speaker text-to-speech (TTS) model that can generate realistic and engaging speech for a wide range of applications, even without having to train it on a large amount of data. It achieves this by using a number of modifications to the VITS TTS model, including external speaker embeddings, a multilingual training dataset, and joint training of the speaker encoder and decoder.\n"},{"id":5,"href":"/docs/related_work/","title":"Related Work","section":"Docs","content":" The Evolution of Text-to-Speech and Voice Cloning # Overview # Text-to-speech is a surprisingly ancient fascination; scientists as early as 1779 were building models to emulate the human voice. The 30s saw the developmemt of vocoders, which used electronic synthesizers to reproduce human speech in limited capacity. Development on TTS technology remained within the realm of computational and mathematical techniques for speech synthesis until the emergence of deep-learning based technqiues, which improved the quality and naturalness of synthesized voices dramtically. Deepfaked audios, as part of this evolution, have thus developed as a result of advancements in deep neural networks and related techniques.\nWe summarize below the most popular deepfake tools:\nTool Key Features Lyrebird • Fast • Can produce 1000 sentences per second Char2Wave End-to-end model with 2 components: reader and neural vocoder. Reader is an encoder-decoder model with attention. WaveNet Based on PixleCNN Deep Voice 3 Has 3 components: encoder, decoder, convertor VITS Parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models YourTTS • Based on VITS model • Supports multi-speaker, multilingual speech NeMo State-of-the-art neural TTS where both cascaded and end-to-end (upcoming) systems are included Text-to-Speech: Basic Mechanism # Step 1: Phenomizer\nPhonemizer transforms text into phonemes. Phonemes are a textual representation of the pronunciation of words (for example — the word tomato will have different phonemes in an American and British accent), and this representation helps the downstream model achieve better results. No machine learning occurs here. Step 2: Acoustic Features\nAn acoustic model transforms phonemes into a Mel spectrogram. Mel Spectrogram is a representation of audio in the time X frequency domain. A spectrogram is achieved by applying a short Fourier transform (STFT) on overlapping time windows of a raw audio waveform. Typically, a trained model is used to generate acoustic features from input text; these features represent the spectral characteristics of speech and include information about the fundamental frequency (F0), spectral envelope, and duration. Many advanced pieces of information are captured here, like intonation, rhythm and stress patterns, and depending on what information is captured by the model, it can be possible to handle speaker variability. Step 3: Vocoder Model / Waveform Synthesis\nThe Mel Spectrogram is converted into a waveform Waveform is sampled at 24/48 kHz, where each sample is digitized into a 16-bit number. These numbers represent the amount of air pressure at each moment in time, which is the sound we eventually hear in our ears. From \u0026ldquo;A Survey on Neural Speech Synthesis\u0026rdquo; at https://doi.org/10.48550/arXiv.2106.15561. Models used for each step are shown.\n"},{"id":6,"href":"/docs/references/","title":"References","section":"Docs","content":" References # *Draft* # [1] A. van den Oord et al., “WaveNet: A Generative Model for Raw Audio,” 2016, doi: 10.48550/ARXIV.1609.03499.\n[2] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High Fidelity Neural Audio Compression,” 2022, doi: 10.48550/ARXIV.2210.13438.\n[3] W. Ping et al., “Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,” 2017, doi: 10.48550/ARXIV.1710.07654.\n[4] X. Tan, T. Qin, F. Soong, and T.-Y. Liu, “A Survey on Neural Speech Synthesis,” 2021, doi: 10.48550/ARXIV.2106.15561.\n[5] J. M. R. Sotelo et al., “Char2Wav: End-to-End Speech Synthesis,” Feb. 2017. Accessed: Nov. 10, 2023. [Online]. Available: https://www.semanticscholar.org/paper/Char2Wav%3A-End-to-End-Speech-Synthesis-Sotelo-Mehri/9203d6c076bffe87336f2ea91f5851436c02dbe6\nDatasets # VCTK LibriSpeech ASR Corpus More Resources # Concatenative speech synthesis Hugging Face - TTS Deep Dive Microsoft Research - Pushing the frontier of neural text to speech John Tan Chong Min - Using Transformers to mimic anyone\u0026rsquo;s voice! Aleksa Gordić - High Fidelity Neural Audio Compression "},{"id":7,"href":"/docs/team/","title":"Meet the Team","section":"Docs","content":" Meet the Team # *Draft* # TODO: Images, linkedin contact and info\nAs part of a collboration between SLAC, CMU and Stanford.\nSLAC # Sponsor point(s) of contact: Mayank Malik (mayank13@stanford.edu)\nCMU Faculty # Special Thanks to CMU Faculty\nDavid Varodayan (dvaroday@andrew.cmu.edu) Cynthia Kuo Sujata Telang Gladys Mercier CMU Student Team # Technical lead(s): Nivea Sharma (niveas@andrew.cmu.edu) Engineering Team:\nHao Ren (haoren@andrew.cmu.edu) Sivani Papini (spapini@andrew.cmu.edu) Thenuga Priyadharshini (tpriyadh@andrew.cmu.edu) Tianyi Li (tianyil3@andrew.cmu.edu) Project manager: Yixin (Sherrie) Cao (yixinc@andrew.cmu.edu)\nRoles + responsibilities:\nSponsors bring domain expertise and real-world problems to the table. Sponsors guide the direction of the project. CMU faculty advisors serve as another resource for the student team. Faculty advisors may bring domain expertise, may help translate sponsors\u0026rsquo; needs for the students, may help with refining student presentations, or may coach individual students to work more effectively. Project managers keep teams on track to meet their project milestones. Project managers coordinate activities between team members and manage communications (e.g., for scheduling meetings) between the sponsors and students. III students spend ~12 hours/week on practicum, which includes project work, team meetings, and class time. Technical leads drive the teams\u0026rsquo; technical discussions. Technical leads need to think ahead to ensure that individual team members\u0026rsquo; work will integrate cleanly and that edge cases are covered. Technical leads are also individual contributors to their projects. Engineers are responsible for the technical development work on the project. INI students spend ~24 hours/week on practicum, which includes project work, team meetings, and class time. "}]