[{"id":0,"href":"/docs/introduction/","title":"Introduction","section":"Docs","content":" Introduction # GitHub\nAI is becoming more powerful and accessible.\nDeepFake audio (AI Synthetic Voice) is becoming a huge threat to various aspects of our society, as it poses significant threats to privacy, security and integrity of information. A synthetic voice generated by AI may impersonate someone, and thus benefit illegal actions and result in undesired outcomes. In this project, we focus on a specific use case, where energy grids are becoming increasingly vulnerable to cyber attacks, especially during extreme weather events. One way that attackers can exploit this vulnerability is by streaming synthetically generated media to operators in the field. This media can be used to trick the operators into taking actions that could damage the grid or compromise its security.\nThere is a lack of awareness of this threat, and therefore, there are no established methods for detecting synthetically generated media for this use case. This is due in part to a scarcity of proper synthetic media datasets, which are needed to train machine learning models for detection.\nTo address this gap, we propose to build a fully operationalized DeepFake audio system using AWS, and train an AL synthetic audio model. Moreover, we hope to acknowledge the pressing scarcity of authentic synthetic media datasets, a significant impediment for scientists, developers, and engineers diligently working on DeepFake detection solutions. In response, we propose utilizing our state-of-the-art DeepFake audio system to generate a meticulously labeled DeepFake audio dataset. We want to showcase the ability of the model to impersonate someoneâ€™s voices to scientists and engineers. By demonstrating the ease with which a malicious actor may be able to impersonate someone\u0026rsquo;s voice, we hope to bring awareness about this class of cyber threats, and the potential of deepfakes to aid and abet such threats. This can inspire caution into people who are affected by the use of DeepFakes in energy grids, such as field operators. We also hope to encourage more research and development into mitigation efforts for this threat.\nMoreover, proper synthetic media datasets are scarce for scientists, developers and engineers to work on detection of DeepFake later. Hence, we propose to use our DeepFake audio system to generate a labeled DeepFake audio dataset. By making this invaluable resource available, we empower scientists and engineers to develop robust detection methodologies rooted in real-world examples, thus fortifying our collective defenses against the pervasive threat of synthetic media.\nYou can find audio samples generated by our system here and find our GitHub here.\n"},{"id":1,"href":"/docs/team/","title":"Meet the Team","section":"Docs","content":" Meet the Team # We are trying to generate synthetic audios!\nVisit our github.\n"},{"id":2,"href":"/docs/research/","title":"Research","section":"Docs","content":" Research # We are trying to generate synthetic audios!\nVisit our github..\n"},{"id":3,"href":"/docs/samples/","title":"Samples","section":"Docs","content":" Samples # XTTS and VALL-E-X # Speaker Prompt VALL-E X XTTS hao_31_24000.mp4 hao_vallex.mp4 hao_xtts.mp4 amused.mp4 vallex-amused.mp4 xtts-amused.mp4 anger.mp4 vallex-anger.mp4 xtts-anger.mp4 disgust.mp4 vallex-amused.mp4 xtts-amused.mp4 muffled.mp4 vallex-muffled.mp4 xtts-muffled.mp4 libri.mp4 libri-vallex.mp4 libri-xtts.mp4 vctk.mp4 vctk-vallex.mp4 vctk-xtts.mp4 "}]