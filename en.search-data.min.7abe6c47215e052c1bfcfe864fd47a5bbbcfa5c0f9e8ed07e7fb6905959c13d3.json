[{"id":0,"href":"/docs/introduction/","title":"Introduction","section":"Docs","content":" Introduction # GitHub\nAI is becoming more powerful and accessible.\nDeepFake audio (AI Synthetic Voice) is becoming a huge threat to various aspects of our society, as it poses significant threats to privacy, security and integrity of information. A synthetic voice generated by AI may impersonate someone, and thus benefit illegal actions and result in undesired outcomes. In this project, we focus on a specific use case, where energy grids are becoming increasingly vulnerable to cyber attacks, especially during extreme weather events. One way that attackers can exploit this vulnerability is by streaming synthetically generated media to operators in the field. This media can be used to trick the operators into taking actions that could damage the grid or compromise its security.\nThere is a lack of awareness of this threat, and therefore, there are no established methods for detecting synthetically generated media for this use case. This is due in part to a scarcity of proper synthetic media datasets, which are needed to train machine learning models for detection.\nTo address this gap, we propose to build a fully operationalized DeepFake audio system using AWS, and train an AL synthetic audio model. Moreover, we hope to acknowledge the pressing scarcity of authentic synthetic media datasets, a significant impediment for scientists, developers, and engineers diligently working on DeepFake detection solutions. In response, we propose utilizing our state-of-the-art DeepFake audio system to generate a meticulously labeled DeepFake audio dataset. We want to showcase the ability of the model to impersonate someoneâ€™s voices to scientists and engineers. By demonstrating the ease with which a malicious actor may be able to impersonate someone\u0026rsquo;s voice, we hope to bring awareness about this class of cyber threats, and the potential of deepfakes to aid and abet such threats. This can inspire caution into people who are affected by the use of DeepFakes in energy grids, such as field operators. We also hope to encourage more research and development into mitigation efforts for this threat.\nMoreover, proper synthetic media datasets are scarce for scientists, developers and engineers to work on detection of DeepFake later. Hence, we propose to use our DeepFake audio system to generate a labeled DeepFake audio dataset. By making this invaluable resource available, we empower scientists and engineers to develop robust detection methodologies rooted in real-world examples, thus fortifying our collective defenses against the pervasive threat of synthetic media.\nYou can find audio samples generated by our system here and find our GitHub here.\n"},{"id":1,"href":"/docs/team/","title":"Meet the Team","section":"Docs","content":" Meet the Team # We are trying to generate synthetic audios!\nVisit our github.\n"},{"id":2,"href":"/docs/research/","title":"Research","section":"Docs","content":" Research # VALL-E\u2028# VALL-E is a neural codec language model for text-to-speech (TTS) synthesis that can generate high-quality speech that is indistinguishable from human speech. It is trained on a massive dataset of text and audio, and can also clone speakers\u0026rsquo; voices, even from a short sample of their speech. VALL-E works by first converting text into a sequence of discrete codes. These codes are then used to generate a corresponding sequence of audio tokens. The audio tokens are then decoded into a waveform using a neural audio codec model.\nVALL-E X # VALL-E X is an extension of VALL-E that enables cross-lingual text-to-speech synthesis and speech-to-speech translation. It is trained on a multilingual dataset of text and audio, and can synthesize speech in a target language from a text prompt in the source language, while preserving the speaker\u0026rsquo;s voice, emotion, and acoustic environment.\u2028YourTTS\u2028YourTTS is a zero-shot multi-speaker text-to-speech (TTS) model that can generate realistic and engaging speech for a wide range of applications, even without having to train it on a large amount of data. It achieves this by using a number of modifications to the VITS TTS model, including external speaker embeddings, a multilingual training dataset, and joint training of the speaker encoder and decoder.\nYourTTS # YourTTS has been shown to achieve state-of-the-art results on the VCTK dataset, and it is a promising new approach to zero-shot multi-speaker TTS. Find the paper here.\nXTTS # XTTS is a cross-lingual text-to-speech synthesis and speech-to-speech translation model that is based on VALL-E. It uses a number of modifications to VALL-E, including a multilingual transformer encoder, a cross-lingual attention mechanism, and a multilingual audio codec model. XTTS has been shown to achieve state-of-the-art results on the Multilingual VCTK dataset and has the potential to be used for a variety of applications.\nWe are currently in the process of fine-tuning these models to fit our needs.\n"},{"id":3,"href":"/docs/samples/","title":"Samples","section":"Docs","content":" Samples # XTTS and VALL-E-X # Speaker Prompt VALL-E X XTTS hao_31_24000.mp4 hao_vallex.mp4 hao_xtts.mp4 amused.mp4 vallex-amused.mp4 xtts-amused.mp4 anger.mp4 vallex-anger.mp4 xtts-anger.mp4 disgust.mp4 vallex-amused.mp4 xtts-amused.mp4 muffled.mp4 vallex-muffled.mp4 xtts-muffled.mp4 libri.mp4 libri-vallex.mp4 libri-xtts.mp4 vctk.mp4 vctk-vallex.mp4 vctk-xtts.mp4 "}]