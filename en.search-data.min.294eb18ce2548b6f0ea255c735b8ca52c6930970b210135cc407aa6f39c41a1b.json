[{"id":0,"href":"/docs/deepfakes-history/","title":"Deepfakes, Then and Now","section":"Docs","content":" Deepfakes, Then and Now # Deepfake videos and images that convincingly replace one person\u0026rsquo;s likeness with another\u0026rsquo;s, have rapidly grown in popularity and sophistication. The generation of deepfakes for both benign and nefarious purposes has only become more accessible through apps like FakeApp, Synthesia, Deepfakesweb.com and more. As a result, research into deepfakes has surged, driven by the need to understand, detect, and mitigate the potential threats they pose.\nSeveral key factors drive increased interest in deepfakes research:\nAdvancements in AI and Deep Learning: Breakthroughs in artificial intelligence and deep learning have made it easier to create highly convincing deepfakes, necessitating a parallel advancement in detection and prevention methods. High-Profile Incidents: Deepfake-related incidents involving public figures and politicians have heightened awareness about their potential for malicious use, prompting more researchers to address these concerns. Privacy Concerns: As deepfake technology can be misused to create non-consensual explicit content or deceive individuals, privacy advocates have called for more research and legislation to combat these issues. National Security: Governments and security agencies have recognized the potential threat deepfakes pose to national security and infrastructure, leading to increased funding for research efforts to develop countermeasures. The landscape for deepfake research is expanding, taking root in disciplines outside of pure information and computer science:\nMitigation strategies for the proliferation of deepfakes requires a potentially interdisciplinary approach, seeing that it bleeds into every facet of society:\nDetection and Authentication: Researchers are developing tools and algorithms to identify deepfake content, often based on discrepancies in facial movements, blinking patterns, or audio inconsistencies. Policy and Legislation: Scholars are investigating the legal and ethical implications of deepfakes and advocating for regulations to prevent their misuse. Media Literacy and Enhanced Communication Skills: Efforts are underway to educate the public about deepfakes and provide tools for recognizing them, enhancing digital literacy. Erosion in public trust of important news sources and previously established language and communication rules is extremely detrimental, and deepfakes only exacerbate the issue. Countermeasures: Technological countermeasures, such as watermarking and authentication techniques, are being developed to hinder the creation and distribution of malicious deepfake content. Cross-Disciplinary Collaboration: Researchers from fields as diverse as computer science, psychology, and ethics are joining forces to address the multifaceted challenges posed by deepfakes. The rise of deepfake technology has prompted a commensurate increase in research, as the potential consequences of its misuse become more apparent. Deepfake research is essential to stay ahead of malicious uses and protect privacy, security, and societal trust. As this technology continues to advance, collaboration between academia, industry, and government will be vital in our ongoing battle against the deepfake phenomenon.\n"},{"id":1,"href":"/docs/our-research/","title":"Our Research","section":"Docs","content":" Our Research # VITS # VITS is a previous SOTA model. It is the first end-to-end model compare to previous 2-stage model, meaning there is no need to produce intermediate mel-spectrums.\nOur fine-tuning code is based on this repo.\nThe repo has a pretrained model on VCTK and some Animes. It can be used for fine-tuning Chinese, Japanese and English.\nWe self recorded a tiny collection of voices, and tried to fine-tune pretrained VITS model based on this dataset. Besides, we also tried public dataset like LibriSpeech, and publically acquired voice samples like Anne Hathaway\u0026rsquo;s speech. Our experiment records are shown as follow:\nDataset Epoch Result* Note Hao\u0026rsquo;s self collected voices (19 Samples) 20 Tune close, pronounciation fail n/a Hao\u0026rsquo;s self collected voices (41 Samples) 25 No improvement n/a Anne Hathaway, 12 mins audio 20 Tune close, pronounciation better, not perfect n/a Hao\u0026rsquo;s self collected voices (15 mins audio, Chinese) 50 Tune close, still strange pronouns, but better than EN n/a LibriSpeech(Speaker 3572, 103 samples) 50 Pretty steady n/a *Since performance of synthetic audios are usually evaluated by humans (like MOS), result column records our experience with the output audio.\nWe found that the quality of the fine-tuned model is highly dependend on number of data samples. A successfully fine-tuned model needs hours of audio samples.\nYourTTS # A newer ZS-TTS model based on VITS, proposed by Coqui. It is also multi-lingual. We implemented our fine-tuning based on the open-source code released by Coqui.\nDataset Epoch Result Note Hao\u0026rsquo;s self collected voices (19 Samples) 20 fail, voice not clearly formed n/a Hao\u0026rsquo;s self collected voices (19 Samples) 160 fail, voice not clearly formed n/a We did not dig deeper into the training, as we found VALL-E model later and shifted our priority to VALL-E model.\nVALL-E # Microsoft\u0026rsquo;s new SOTA ZS-TTS model that proposed a different way to perform the task. It tokenize the audio prompt and text prompt into T * 8 matrix, and then feed them into an Auto-regressive model and a Non-autogressive model.\nFor each model, the basic strcuture contains several embedding layers, and a Transformer Decoder layer.\nSince Microsoft does not publish VALL-E\u0026rsquo;s code, we tried several publically avaiable implementation of Microsoft VALL-E, described in following sections.\nenhuiz # This repo uses g2p (Grapheme-to-phoneme conversion) to quantize and tokenize dataset. It also uses Deepspeed to speed up training.\nThe main issue with this method is that there is no pretrained model. So we tried to train a backbone model using LibriLight, and the fine-tune the backbone model using self-collected smaller dataset.\nProbelms:\nTraining a sophisticated backbone model ourselves requires way more time and knowledge. LibriLight is too small (1200 samples) for backbone model. Directly trained on self-collected dataset, gives toy result as exepcted. Ecker # Ecker\u0026rsquo;s code is based on enhuiz\u0026rsquo;s scripts, but the author released a pretrained model. So we thought we can conduct our fine-tuning based on the pretrained model.\nProblems:\npretrained model has error, it only gives nan result in AR forward. We also checked on huggingface, the model is not working. Training based on the pretrained model has a high loss. Lifeiteng # Lifeiteng\u0026rsquo;s implementation relies on Lhotse for its dataset preparation. Lhotse will process the audios into CutSet, and later a DynamicBucketSampler is used for constructing the dataloader.\nOne interesting bug we found is that we need to change our dataset to mono-channel, since Encodec uses 24KHZ mono-channel. If stereo inputs are given, we need to use 49KHZ Encodec model.\nNo pretrained model is released for Lifeiteng\u0026rsquo;s repo. We found a pioneer who trained on LibriTTS for 100 epoches, and continued our fine-tuning based on his checkpoint.\nProblem:\nThe checkpoint performance is very bad. On AWS, we met complex version problem brought by torch, nnCUDA, K2 and python version. Seems no easy workable solution can be found among them. Plachtaa # Checkout our fine-tune branch.\nPlachtaa uses Lifeiteng\u0026rsquo;s repo, and releases a super well-trained pretrained model. The only problem is that Plachtaa deleted all traing related codes, like dataset processing, training script and even model\u0026rsquo;s forward pass.\nThere are differences between Plachtaa and Lifeiteng\u0026rsquo;s implementation: Plachtaa supports multi-lingual by adding language embedding, uses different tokenizer and trained on different dataset.\nWe tried to re-write all these missing training codes, and runned our fine-tuning. Following sections summaries our experiments. We are still facing inference issue as generated audio is mainly noise, actively solving it now.\nImplementation details # As lifeiteng uses Lhotse, we have to make our dataset something like libriTTS. We also need to use its CutSet as dataset class, and its Sampler when constructing dataloader.\nLifeiteng and Plachtaa uses different tokenizer, resulting difference in TextTokenCollator\nLifeiteng Tokenize logic: text_prompt -\u0026gt; tokenizer (espeak) -\u0026gt; phonemes -\u0026gt; Collator (using precomputed SymbolTable) -\u0026gt; index sequence\nPlachtaa Tokenize logic: text_prompt -\u0026gt; Add language ID -\u0026gt; tokenizer (PhonemeBpeTokenizer, with pre-defined index mapping) -\u0026gt; index sequence directly, Collator just need to perform EOS/DOS/PAD\nThe most important thing to notice is that Plachtaa added language ID to text prompt:\ntext = \u0026#34;[EN]\u0026#34; + text + \u0026#34;[EN]\u0026#34; Besides, phoneme index mapping is different. For example, PAD in PhonemeBpeTokenizer is 3, but in Lifeiteng\u0026rsquo;s Tokenizer is 0.\nIn the model\u0026rsquo;s forward pass, we added language embedding calculation accordingly.\nFirst run # Successfully trained on Plachtaa\u0026rsquo;s pretrained model, using Lifeiteng\u0026rsquo;s training code. Several key things to notice:\nPrepare our dataset in LibriTTS format Using Lifeiteng\u0026rsquo;s VALLE forward, but added language embedding Fixed embedding size in order to load Plachtaa\u0026rsquo;s pretrained model Fixed erros in Checkpoint, training logic, data processing (collation), etc. Fixed inference script: it correctly produces good result if we directly use pretrained model Run 2 # learning rate 1e-5 warmup-epochs 0 Trained AR, epoch 80, keep ar_predict_layer unfreezed log_interval, valid_interval, checkpoint frequency and logic Phase loss Top10Accuracy Train 4.657 0.5279 Valid 4.909 0.4687 Run 3 # Add language id \u0026ldquo;EN\u0026rdquo; to transcription, change to PhonemeBpeTokenizer text tokenizer in prepare stage learning rate 1e-6 warmup-epochs 0 Trained AR, epoch 80, keep ar_predict_layer unfreezed Trained NAR epoch 180, keep nar_predict_layers unfreezed Phase loss Top10Accuracy AR Train 4.967 0.4304 AR Valid 4.914 0.4665 NAR Train 7.999 0.01076 NAR Valid 7.79 0.01838 Run 4 # Fixed wrong embedding used in AR forwarding learning rate 0.04 (ScaledAdam should be smart enough) warmup-epochs 200 Trained AR, epoch 160, keep ar_predict_layer unfreezed Phase loss Top10Accuracy AR Train 4.765 0.4365 AR Valid 4.763 0.4665 Run 5 # Fixed forward logic: removed enrolled_len learning rate 1e-4, ScaledAdam warmup-epochs 200 Trained AR, epoch 1000, keep ar_predict_layer unfreezed Phase loss Top10Accuracy AR Train 4.396 0.4497 AR Valid 4.504 0.4697 Run 6 # libriTTS dataset: dev-clean, test-clean (train cut 08:58:13, dev cut 00:47:31, test cut 08:34:09) learning rate 1e-5, ScaledAdam, warmup-epochs 200 AR, 20 epochs, (did not include validation as valid_interval was wrong) Phase loss Top10Accuracy AR Train 5.748 0.1868 Run 7 # learning rate 0.04, ScaledAdam, warmup-epochs 200 AR 20 epochs, NAR 20 epochs Phase loss Top10Accuracy AR Train 5.748 0.1868 AR Valid 5.923 0.1715 NAR Train 5.669 0.2045 NAR Valid 5.759 0.1858 "},{"id":2,"href":"/docs/team/","title":"Meet the Team","section":"Docs","content":" Meet the Team # *Draft* # TODO: Images, linkedin contact and info\nAs part of a collboration between SLAC, CMU and Stanford.\nSLAC # Sponsor point(s) of contact: Mayank Malik (mayank13@stanford.edu)\nCMU Faculty # Special Thanks to CMU Faculty\nDavid Varodayan (dvaroday@andrew.cmu.edu) Cynthia Kuo Sujata Telang Gladys Mercier CMU Student Team # Technical lead(s): Nivea Sharma (niveas@andrew.cmu.edu) Engineering Team:\nHao Ren (haoren@andrew.cmu.edu) Sivani Papini (spapini@andrew.cmu.edu) Thenuga Priyadharshini (tpriyadh@andrew.cmu.edu) Tianyi Li (tianyil3@andrew.cmu.edu) Project manager: Yixin (Sherrie) Cao (yixinc@andrew.cmu.edu)\nRoles + responsibilities:\nSponsors bring domain expertise and real-world problems to the table. Sponsors guide the direction of the project. CMU faculty advisors serve as another resource for the student team. Faculty advisors may bring domain expertise, may help translate sponsors\u0026rsquo; needs for the students, may help with refining student presentations, or may coach individual students to work more effectively. Project managers keep teams on track to meet their project milestones. Project managers coordinate activities between team members and manage communications (e.g., for scheduling meetings) between the sponsors and students. III students spend ~12 hours/week on practicum, which includes project work, team meetings, and class time. Technical leads drive the teams\u0026rsquo; technical discussions. Technical leads need to think ahead to ensure that individual team members\u0026rsquo; work will integrate cleanly and that edge cases are covered. Technical leads are also individual contributors to their projects. Engineers are responsible for the technical development work on the project. INI students spend ~24 hours/week on practicum, which includes project work, team meetings, and class time. "},{"id":3,"href":"/docs/samples/","title":"Samples","section":"Docs","content":" Samples # XTTS and VALL-E-X # Speaker Prompt VALL-E X XTTS hao_31_24000.mp4 hao_vallex.mp4 hao_xtts.mp4 amused.mp4 vallex-amused.mp4 xtts-amused.mp4 anger.mp4 vallex-anger.mp4 xtts-anger.mp4 disgust.mp4 vallex-amused.mp4 xtts-amused.mp4 muffled.mp4 vallex-muffled.mp4 xtts-muffled.mp4 libri.mp4 libri-vallex.mp4 libri-xtts.mp4 vctk.mp4 vctk-vallex.mp4 vctk-xtts.mp4 "},{"id":4,"href":"/docs/references/","title":"References","section":"Docs","content":" References # *Draft* # TODO: Formatting to apa/mla\nhttps://arxiv.org/pdf/2210.13438.pdf — The Encodec paper (High Fidelity Neural Audio Compression) https://wiki.aalto.fi/display/ITSP/Concatenative+speech+synthesis — Explanation on concatenative speech synthesis https://www.youtube.com/watch?v=aLBedWj-5CQ\u0026t=1s — Deep dive into speech synthesis meetup (HuggingFace) https://www.youtube.com/watch?v=MA8PCvmr8B0 — Pushing the frontier of neural text to speech (Microsoft Research) https://www.youtube.com/watch?v=G9k-2mYl6Vo\u0026t=5593s — Excellent video by John Tan Chong Min about VALL-E https://www.youtube.com/watch?v=mV7bhf6b2Hs — Excellent video by Aleksa Gordic about Encodec\n"}]